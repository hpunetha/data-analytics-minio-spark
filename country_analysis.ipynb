{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import s3fs\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables\n",
    "os.environ[\"MINIO_KEY\"] = \"minio\"\n",
    "os.environ[\"MINIO_SECRET\"] = \"minio123\"\n",
    "os.environ[\"MINIO_ENDPOINT\"] = \"http://minio1:9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data using REST API\n",
    "def fetch_countries_data(url):\n",
    "    # Using session is particularly beneficial \n",
    "    # if you are making multiple requests to the same server, \n",
    "    # as it can reuse the underlying TCP connection, \n",
    "    # leading to performance improvements.\n",
    "    with requests.Session() as session:\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "# Fetch data\n",
    "countries_data = fetch_countries_data(\"https://restcountries.com/v3.1/all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to minIO as a JSON file\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': os.environ[\"MINIO_ENDPOINT\"]}, # minio1 = minio container name\n",
    "    key=os.environ[\"MINIO_KEY\"],\n",
    "    secret=os.environ[\"MINIO_SECRET\"],\n",
    "    use_ssl=False  # Set to True if MinIO is set up with SSL\n",
    ")\n",
    "\n",
    "with fs.open('mybucket/country_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(countries_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "\n",
    "\"\"\"\n",
    "- `spark.hadoop.fs.s3a.endpoint`: The endpoint URL for minIO.\n",
    "- `spark.hadoop.fs.s3a.access.key` and `spark.hadoop.fs.s3a.secret.key`: The access key and secret key for minIO.\n",
    "- `spark.hadoop.fs.s3a.path.style.access`: Set to true to enable path-style access for S3 bucket.\n",
    "- `spark.hadoop.fs.s3a.impl`: The implementation class for S3A file system.\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"country_data_analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_KEY\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON data using PySpark\n",
    "\n",
    "df = spark.read.option(\"inferSchema\",True).json(\"s3a://mybucket/country_data.json\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write same data as Parquet and re-read in dataframe\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"parquet\").save(\"s3a://mybucket/country_raw_data.parquet\")\n",
    "country_raw_data = spark.read.parquet(\"s3a://mybucket/country_raw_data.parquet\")\n",
    "country_raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform transformations to raw data\n",
    "country_trnsfm_data = (\n",
    "    country_raw_data\n",
    "    .selectExpr(\n",
    "        \"name.common as cntry_name\",\n",
    "        \"area as cntry_area\",\n",
    "        \"borders as border_cntry\",\n",
    "        \"capital as capital_cities\",\n",
    "        \"continents as cntry_continent\",\n",
    "        \"landlocked as is_landlocked\",\n",
    "        \"population\",\n",
    "        \"startOfWeek\",\n",
    "        \"timezones as nr_timezones\",\n",
    "        \"unMember as is_unmember\"\n",
    "    )\n",
    "    .withColumn(\"cntry_area\",F.when(F.col(\"cntry_area\") < 0, None).otherwise(F.col(\"cntry_area\")))\n",
    "    .withColumn(\"border_cntry\",F.when(F.col(\"border_cntry\").isNull(),F.array(F.lit(\"NA\"))).otherwise(F.col(\"border_cntry\")))\n",
    "    .withColumn(\"capital_cities\",F.when(F.col(\"capital_cities\").isNull(),F.array(F.lit(\"NA\"))).otherwise(F.col(\"capital_cities\")))    \n",
    ")\n",
    "\n",
    "# Print schema of transformed data\n",
    "country_trnsfm_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transformed data as PARQUET\n",
    "country_trnsfm_data.write.mode(\"overwrite\").format(\"parquet\").save(\"s3a://mybucket/country_trnsfm_data.parquet\")\n",
    "\n",
    "# Create external hive table using PARQUET\n",
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE country_data (\n",
    "    cntry_name STRING,\n",
    "    cntry_area DOUBLE,\n",
    "    border_cntry ARRAY<STRING>,\n",
    "    capital_cities ARRAY<STRING>,\n",
    "    cntry_continent ARRAY<STRING>,\n",
    "    is_landlocked BOOLEAN,\n",
    "    population BIGINT,\n",
    "    startOfWeek STRING,\n",
    "    nr_timezones ARRAY<STRING>,\n",
    "    is_unmember BOOLEAN\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3a://mybucket/country_trnsfm_data.parquet';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table details\n",
    "spark.sql(\"DESCRIBE EXTENDED default.country_data\").show(100,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 records from the table\n",
    "spark.sql(\"SELECT * FROM default.country_data LIMIT 5\").show(truncate = False)\n",
    "\n",
    "# Create temporary view using dataframe\n",
    "spark.table(\"default.country_data\").createOrReplaceTempView(\"country_data_processed_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show Spark SQL results\n",
    "def show_results(sql_string):\n",
    "    return spark.sql(\n",
    "        sql_string\n",
    "    ).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Which are the 10 largest countries in terms of area? (in sq. km.)\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, cntry_area\n",
    "    FROM country_data_processed_view\n",
    "    ORDER BY cntry_area DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Which country has the largest number of neighbouring countries?\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, border_cntry, array_size(border_cntry) as ngbr_cntry_nr\n",
    "    FROM country_data_processed_view\n",
    "    WHERE NOT array_contains(border_cntry,'NA')\n",
    "    ORDER BY array_size(border_cntry) DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Which countries have the highest number of capital cities?\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, capital_cities, array_size(capital_cities) as total_capital_cities\n",
    "    FROM country_data_processed_view\n",
    "    WHERE NOT array_contains(capital_cities,'NA')\n",
    "    ORDER BY array_size(capital_cities) DESC\n",
    "    LIMIT 2\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How many countries lie on two or more continents?\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, cntry_continent, array_size(cntry_continent) as total_continents\n",
    "    FROM country_data_processed_view\n",
    "    ORDER BY array_size(cntry_continent) DESC\n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. How many landlocked countries per continent?\n",
    "sql_string = \"\"\"\n",
    "    SELECT continent, SUM(is_landlocked) as landlocked_nr\n",
    "    FROM (SELECT cntry_name, case when is_landlocked then 1 else 0 end as is_landlocked, explode(cntry_continent) as continent\n",
    "    FROM country_data_processed_view)\n",
    "    GROUP BY continent\n",
    "    ORDER BY SUM(is_landlocked) DESC\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Which country has the highest number of time zones?\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, nr_timezones, array_size(nr_timezones) as total_timezones\n",
    "    FROM country_data_processed_view\n",
    "    ORDER BY array_size(nr_timezones) DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. How many countries are not UN members?\n",
    "sql_string = \"\"\"\n",
    "    SELECT COUNT(*) AS count\n",
    "    FROM country_data_processed_view\n",
    "    WHERE NOT is_unmember\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
