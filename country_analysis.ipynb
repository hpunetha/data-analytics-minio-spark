{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f100e096-d49f-465d-9f62-43c3098366af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs)\n",
      "  Downloading aiobotocore-2.9.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting fsspec==2023.12.2 (from s3fs)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from s3fs)\n",
      "  Downloading aiohttp-3.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting botocore<1.33.14,>=1.33.2 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading botocore-1.33.13-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.33.14,>=1.33.2->aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.33.14,>=1.33.2->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.33.14,>=1.33.2->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.0.7)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.33.14,>=1.33.2->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Downloading s3fs-2023.12.2-py3-none-any.whl (28 kB)\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiobotocore-2.9.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.33.13-py3-none-any.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.3/272.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.1/328.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wrapt, multidict, jmespath, fsspec, frozenlist, aioitertools, yarl, botocore, aiosignal, aiohttp, aiobotocore, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.9.2\n",
      "    Uninstalling fsspec-2023.9.2:\n",
      "      Successfully uninstalled fsspec-2023.9.2\n",
      "Successfully installed aiobotocore-2.9.0 aiohttp-3.9.1 aioitertools-0.11.0 aiosignal-1.3.1 botocore-1.33.13 frozenlist-1.4.1 fsspec-2023.12.2 jmespath-1.0.1 multidict-6.0.4 s3fs-2023.12.2 wrapt-1.16.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39bc3610-ec62-4e0d-83ae-cb59371b896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import s3fs\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b86922-8d3c-4f36-8d6f-35ebfc2f1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables\n",
    "os.environ[\"MINIO_KEY\"] = \"minio\"\n",
    "os.environ[\"MINIO_SECRET\"] = \"minio123\"\n",
    "os.environ[\"MINIO_ENDPOINT\"] = \"http://minio1:9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ae9b37-f11e-4522-8846-c700e3be21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data using REST API\n",
    "def fetch_countries_data(url):\n",
    "    # Using session is particularly beneficial \n",
    "    # if you are making multiple requests to the same server, \n",
    "    # as it can reuse the underlying TCP connection, \n",
    "    # leading to performance improvements.\n",
    "    with requests.Session() as session:\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "# Fetch data\n",
    "countries_data = fetch_countries_data(\"https://restcountries.com/v3.1/all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065144e4-b01d-41ce-a6f6-c16c66f40a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to minIO as a JSON file\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': os.environ[\"MINIO_ENDPOINT\"]}, # minio1 = minio container name\n",
    "    key=os.environ[\"MINIO_KEY\"],\n",
    "    secret=os.environ[\"MINIO_SECRET\"],\n",
    "    use_ssl=False  # Set to True if MinIO is set up with SSL\n",
    ")\n",
    "\n",
    "with fs.open('mybucket/country_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(countries_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824ce0e4-a612-4fdd-8f20-a40983fdecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "\n",
    "\"\"\"\n",
    "- `spark.hadoop.fs.s3a.endpoint`: The endpoint URL for minIO.\n",
    "- `spark.hadoop.fs.s3a.access.key` and `spark.hadoop.fs.s3a.secret.key`: The access key and secret key for minIO.\n",
    "- `spark.hadoop.fs.s3a.path.style.access`: Set to true to enable path-style access for S3 bucket.\n",
    "- `spark.hadoop.fs.s3a.impl`: The implementation class for S3A file system.\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"country_data_analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_KEY\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508bb83c-f58a-42a6-942f-9a9adb9aea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Check PySpark version\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a25e29bc-62da-4521-9c54-eb39ed867ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version: 3.3.4\n"
     ]
    }
   ],
   "source": [
    "# Check Hadoop version\n",
    "sc = SparkContext.getOrCreate()\n",
    "hadoop_version = sc._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion()\n",
    "print(\"Hadoop version:\", hadoop_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d90ce89-dea5-4019-bfc8-8ea34b9c2c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read JSON data using PySpark\n",
    "\n",
    "df = spark.read.option(\"inferSchema\",True).json(\"s3a://mybucket/country_data.json\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979bf18c-f22d-44e7-b36d-7d481f091227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write same data as Parquet and re-read in dataframe\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"parquet\").save(\"s3a://mybucket/country_raw_data.parquet\")\n",
    "country_raw_data = spark.read.parquet(\"s3a://mybucket/country_raw_data.parquet\")\n",
    "country_raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed87274b-91e2-45ee-8123-63e03d9f1d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cntry_name: string (nullable = true)\n",
      " |-- cntry_area: double (nullable = true)\n",
      " |-- border_cntry: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- capital_cities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cntry_continent: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- is_landlocked: boolean (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      " |-- startOfWeek: string (nullable = true)\n",
      " |-- nr_timezones: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- is_unmember: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform transformations to raw data\n",
    "country_trnsfm_data = (\n",
    "    country_raw_data\n",
    "    .selectExpr(\n",
    "        \"name.common as cntry_name\",\n",
    "        \"area as cntry_area\",\n",
    "        \"borders as border_cntry\",\n",
    "        \"capital as capital_cities\",\n",
    "        \"continents as cntry_continent\",\n",
    "        \"landlocked as is_landlocked\",\n",
    "        \"population\",\n",
    "        \"startOfWeek\",\n",
    "        \"timezones as nr_timezones\",\n",
    "        \"unMember as is_unmember\"\n",
    "    )\n",
    "    .withColumn(\"cntry_area\",F.when(F.col(\"cntry_area\") < 0, None).otherwise(F.col(\"cntry_area\")))\n",
    "    .withColumn(\"border_cntry\",F.when(F.col(\"border_cntry\").isNull(),F.array(F.lit(\"NA\"))).otherwise(F.col(\"border_cntry\")))\n",
    "    .withColumn(\"capital_cities\",F.when(F.col(\"capital_cities\").isNull(),F.array(F.lit(\"NA\"))).otherwise(F.col(\"capital_cities\")))    \n",
    ")\n",
    "\n",
    "# Print schema of transformed data\n",
    "country_trnsfm_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75a8fe8e-b648-47a8-a8bd-1b27807a5805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write transformed data as PARQUET\n",
    "country_trnsfm_data.write.mode(\"overwrite\").format(\"parquet\").save(\"s3a://mybucket/country_trnsfm_data.parquet\")\n",
    "\n",
    "# Create external hive table using PARQUET\n",
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE country_data (\n",
    "    cntry_name STRING,\n",
    "    cntry_area DOUBLE,\n",
    "    border_cntry ARRAY<STRING>,\n",
    "    capital_cities ARRAY<STRING>,\n",
    "    cntry_continent ARRAY<STRING>,\n",
    "    is_landlocked BOOLEAN,\n",
    "    population BIGINT,\n",
    "    startOfWeek STRING,\n",
    "    nr_timezones ARRAY<STRING>,\n",
    "    is_unmember BOOLEAN\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3a://mybucket/country_trnsfm_data.parquet';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b271b2fb-15fb-4705-8fb5-3a172f5ffbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|cntry_name                  |string                                                        |NULL   |\n",
      "|cntry_area                  |double                                                        |NULL   |\n",
      "|border_cntry                |array<string>                                                 |NULL   |\n",
      "|capital_cities              |array<string>                                                 |NULL   |\n",
      "|cntry_continent             |array<string>                                                 |NULL   |\n",
      "|is_landlocked               |boolean                                                       |NULL   |\n",
      "|population                  |bigint                                                        |NULL   |\n",
      "|startOfWeek                 |string                                                        |NULL   |\n",
      "|nr_timezones                |array<string>                                                 |NULL   |\n",
      "|is_unmember                 |boolean                                                       |NULL   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Catalog                     |spark_catalog                                                 |       |\n",
      "|Database                    |default                                                       |       |\n",
      "|Table                       |country_data                                                  |       |\n",
      "|Owner                       |jovyan                                                        |       |\n",
      "|Created Time                |Tue Dec 19 14:10:59 UTC 2023                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.5.0                                                   |       |\n",
      "|Type                        |EXTERNAL                                                      |       |\n",
      "|Provider                    |hive                                                          |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1702995059]                            |       |\n",
      "|Statistics                  |13970 bytes                                                   |       |\n",
      "|Location                    |s3a://mybucket/country_trnsfm_data.parquet                    |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "|Storage Properties          |[serialization.format=1]                                      |       |\n",
      "|Partition Provider          |Catalog                                                       |       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show table details\n",
    "spark.sql(\"DESCRIBE EXTENDED default.country_data\").show(100,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71894eb-1140-45f1-8629-e4e9d0c065da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-------------------------+------------------+---------------+-------------+----------+-----------+------------+-----------+\n",
      "|cntry_name      |cntry_area|border_cntry             |capital_cities    |cntry_continent|is_landlocked|population|startOfWeek|nr_timezones|is_unmember|\n",
      "+----------------+----------+-------------------------+------------------+---------------+-------------+----------+-----------+------------+-----------+\n",
      "|Christmas Island|135.0     |[NA]                     |[Flying Fish Cove]|[Asia]         |false        |2072      |monday     |[UTC+07:00] |false      |\n",
      "|Eritrea         |117600.0  |[DJI, ETH, SDN]          |[Asmara]          |[Africa]       |false        |5352000   |monday     |[UTC+03:00] |true       |\n",
      "|Samoa           |2842.0    |[NA]                     |[Apia]            |[Oceania]      |false        |198410    |monday     |[UTC+13:00] |true       |\n",
      "|North Macedonia |25713.0   |[ALB, BGR, GRC, UNK, SRB]|[Skopje]          |[Europe]       |true         |2077132   |monday     |[UTC+01:00] |true       |\n",
      "|Djibouti        |23200.0   |[ERI, ETH, SOM]          |[Djibouti]        |[Africa]       |false        |988002    |monday     |[UTC+03:00] |true       |\n",
      "+----------------+----------+-------------------------+------------------+---------------+-------------+----------+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first 5 records from the table\n",
    "spark.sql(\"SELECT * FROM default.country_data LIMIT 5\").show(truncate = False)\n",
    "\n",
    "# Create temporary view using dataframe\n",
    "spark.table(\"default.country_data\").createOrReplaceTempView(\"country_data_processed_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "782713f7-981d-45f0-afe8-124ab1e3e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show Spark SQL results\n",
    "def show_results(sql_string):\n",
    "    return spark.sql(\n",
    "        sql_string\n",
    "    ).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5edccf2-e4e7-44f4-9778-4a7a7786e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|cntry_name   |cntry_area |\n",
      "+-------------+-----------+\n",
      "|Russia       |1.7098242E7|\n",
      "|Antarctica   |1.4E7      |\n",
      "|Canada       |9984670.0  |\n",
      "|China        |9706961.0  |\n",
      "|United States|9372610.0  |\n",
      "|Brazil       |8515767.0  |\n",
      "|Australia    |7692024.0  |\n",
      "|India        |3287590.0  |\n",
      "|Argentina    |2780400.0  |\n",
      "|Kazakhstan   |2724900.0  |\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Which are the 10 largest countries in terms of area? (in sq. km.)\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, cntry_area\n",
    "    FROM country_data_processed_view\n",
    "    ORDER BY cntry_area DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb0d452-892c-497c-ab55-a544543c2be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------------------------------------------------+-------------+\n",
      "|cntry_name|border_cntry                                                                    |ngbr_cntry_nr|\n",
      "+----------+--------------------------------------------------------------------------------+-------------+\n",
      "|China     |[AFG, BTN, MMR, HKG, IND, KAZ, NPL, PRK, KGZ, LAO, MAC, MNG, PAK, RUS, TJK, VNM]|16           |\n",
      "+----------+--------------------------------------------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Which country has the largest number of neighbouring countries?\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, border_cntry, array_size(border_cntry) as ngbr_cntry_nr\n",
    "    FROM country_data_processed_view\n",
    "    WHERE NOT array_contains(border_cntry,'NA')\n",
    "    ORDER BY array_size(border_cntry) DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c090aef-d970-45f3-8ae6-a11491702508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------+--------------------+\n",
      "|cntry_name  |capital_cities                     |total_capital_cities|\n",
      "+------------+-----------------------------------+--------------------+\n",
      "|South Africa|[Pretoria, Bloemfontein, Cape Town]|3                   |\n",
      "|Palestine   |[Ramallah, Jerusalem]              |2                   |\n",
      "+------------+-----------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Which countries have the highest number of capital cities?\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, capital_cities, array_size(capital_cities) as total_capital_cities\n",
    "    FROM country_data_processed_view\n",
    "    WHERE NOT array_contains(capital_cities,'NA')\n",
    "    ORDER BY array_size(capital_cities) DESC\n",
    "    LIMIT 2\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6e3278-2c13-42c4-92fe-7e148f8de96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------------+\n",
      "|cntry_name|cntry_continent|total_continents|\n",
      "+----------+---------------+----------------+\n",
      "|Turkey    |[Europe, Asia] |2               |\n",
      "|Azerbaijan|[Europe, Asia] |2               |\n",
      "|Russia    |[Europe, Asia] |2               |\n",
      "+----------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. How many countries lie on two or more continents?\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, cntry_continent, array_size(cntry_continent) as total_continents\n",
    "    FROM country_data_processed_view\n",
    "    ORDER BY array_size(cntry_continent) DESC\n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bbd7da2-7fe0-4ccb-b224-66bd73a15b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|continent    |landlocked_nr|\n",
      "+-------------+-------------+\n",
      "|Europe       |16           |\n",
      "|Africa       |16           |\n",
      "|Asia         |12           |\n",
      "|South America|2            |\n",
      "|North America|0            |\n",
      "|Antarctica   |0            |\n",
      "|Oceania      |0            |\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. How many landlocked countries per continent?\n",
    "sql_string = \"\"\"\n",
    "    SELECT continent, SUM(is_landlocked) as landlocked_nr\n",
    "    FROM (SELECT cntry_name, case when is_landlocked then 1 else 0 end as is_landlocked, explode(cntry_continent) as continent\n",
    "    FROM country_data_processed_view)\n",
    "    GROUP BY continent\n",
    "    ORDER BY SUM(is_landlocked) DESC\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90fa899f-9163-424c-b029-27ac4ca51366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|cntry_name|nr_timezones                                                                                                                                              |total_timezones|\n",
      "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|France    |[UTC-10:00, UTC-09:30, UTC-09:00, UTC-08:00, UTC-04:00, UTC-03:00, UTC+01:00, UTC+02:00, UTC+03:00, UTC+04:00, UTC+05:00, UTC+10:00, UTC+11:00, UTC+12:00]|14             |\n",
      "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Which country has the highest number of time zones?\n",
    "sql_string = \"\"\"\n",
    "    SELECT cntry_name, nr_timezones, array_size(nr_timezones) as total_timezones\n",
    "    FROM country_data_processed_view\n",
    "    ORDER BY array_size(nr_timezones) DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c9ef239-3bba-4fea-8f59-012a3b3dae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|57   |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. How many countries are not UN members?\n",
    "sql_string = \"\"\"\n",
    "    SELECT COUNT(*) AS count\n",
    "    FROM country_data_processed_view\n",
    "    WHERE NOT is_unmember\n",
    "    \"\"\"\n",
    "show_results(sql_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
